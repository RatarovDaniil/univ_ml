{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__UserID, ProfileID, Rating__\n",
    "\n",
    "- __UserID__ is user who provided rating\n",
    "- __ProfileID__ is user who has been rated\n",
    "- __UserIDs__ range between 1 and 135,359\n",
    "- __ProfileIDs__ range between 1 and 220,970 (not every profile has been rated)\n",
    "- __Ratings__ are on a 1-10 scale where 10 is best (integer ratings only)\n",
    "- Only users who provided at least 20 ratings were included\n",
    "- Users who provided constant ratings were excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>ProfileID</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>720</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>971</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1616</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  ProfileID  Ratings\n",
       "0       1        133        8\n",
       "1       1        720        6\n",
       "2       1        971       10\n",
       "3       1       1095        7\n",
       "4       1       1616       10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ratings_data.csv\")\n",
    "df = df.drop([col for col in df.columns if col.startswith('Unnamed')], axis=1)\n",
    "df = df.loc[df['UserID'] <= 1000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "users = list(set(df['UserID']))\n",
    "user_id_index = dict((user_id, index) for user_id, index in zip(users, range(len(users))))\n",
    "items = list(set(df['ProfileID']))\n",
    "item_id_index = dict((item_id, index) for item_id, index in zip(items, range(len(items))))\n",
    "\n",
    "# shuffle dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "old_data = df.values\n",
    "\n",
    "data = np.array([[int(user_id_index[item[0]]), int(item_id_index[item[1]]), int(item[2])] for item in old_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulding a matrix for our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset density: 0.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data\n",
    "ratio = 0.6\n",
    "train_data = data[:int(ratio*data.shape[0])]\n",
    "vali_data = data[int(ratio*data.shape[0]):int((ratio+(1-ratio)/2)*data.shape[0])]\n",
    "test_data = data[int((ratio+(1-ratio)/2)*data.shape[0]):]\n",
    "\n",
    "NUM_USERS = len(set(df['UserID']))\n",
    "NUM_ITEMS = len(set(df['ProfileID']))\n",
    "print('Dataset density: {}%'.format(round(len(df)/(NUM_USERS*NUM_ITEMS)*100, 3)))\n",
    "\n",
    "R = np.zeros([NUM_USERS, NUM_ITEMS])\n",
    "for ele in train_data:\n",
    "    R[ele[0], ele[1]] = float(ele[2])\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(preds, truth):\n",
    "    return np.sqrt(np.mean(np.square(preds-truth)))\n",
    "\n",
    "class PMF():\n",
    "    def __init__(self, R, lambda_alpha=1e-2, lambda_beta=1e-2, latent_size=50, momuntum=0.8,\n",
    "                 lr=0.001, iters=200, seed=None):\n",
    "        # initialize parameters\n",
    "        self.lambda_alpha = lambda_alpha\n",
    "        self.lambda_beta = lambda_beta\n",
    "        self.momuntum = momuntum\n",
    "        self.R = R\n",
    "        self.random_state = RandomState(seed)\n",
    "        self.iterations = iters\n",
    "        self.lr = lr\n",
    "        self.I = copy.deepcopy(self.R)\n",
    "        self.I[self.I != 0] = 1\n",
    "\n",
    "        self.U = 0.1*self.random_state.rand(np.size(R, 0), latent_size)\n",
    "        self.V = 0.1*self.random_state.rand(np.size(R, 1), latent_size)\n",
    "\n",
    "    def loss(self):\n",
    "        # defining loss function\n",
    "        loss = np.sum(self.I*(self.R-np.dot(self.U, self.V.T))**2) + self.lambda_alpha*np.sum(np.square(self.U)) + self.lambda_beta*np.sum(np.square(self.V))\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, data):\n",
    "        # predicting array of values\n",
    "        index_data = np.array([[int(ele[0]), int(ele[1])] for ele in data], dtype=int)\n",
    "        u_features = self.U.take(index_data.take(0, axis=1), axis=0)\n",
    "        v_features = self.V.take(index_data.take(1, axis=1), axis=0)\n",
    "        preds_value_array = np.sum(u_features*v_features, 1)\n",
    "        return preds_value_array\n",
    "\n",
    "    def train(self, train_data=None, vali_data=None):\n",
    "        # training and validating\n",
    "        train_loss_list = []\n",
    "        vali_rmse_list = []\n",
    "        last_vali_rmse = None\n",
    "\n",
    "        momuntum_u = np.zeros(self.U.shape)\n",
    "        momuntum_v = np.zeros(self.V.shape)\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            grads_u = np.dot(self.I*(self.R-np.dot(self.U, self.V.T)), -self.V) + self.lambda_alpha*self.U\n",
    "            grads_v = np.dot((self.I*(self.R-np.dot(self.U, self.V.T))).T, -self.U) + self.lambda_beta*self.V\n",
    "\n",
    "            momuntum_u = (self.momuntum * momuntum_u) + self.lr * grads_u\n",
    "            momuntum_v = (self.momuntum * momuntum_v) + self.lr * grads_v\n",
    "            self.U = self.U - momuntum_u\n",
    "            self.V = self.V - momuntum_v\n",
    "\n",
    "            train_loss = self.loss()\n",
    "            train_loss_list.append(train_loss)\n",
    "\n",
    "            vali_preds = self.predict(vali_data)\n",
    "            vali_rmse = RMSE(vali_data[:,2], vali_preds)\n",
    "            vali_rmse_list.append(vali_rmse)\n",
    "\n",
    "            print('traning iteration:{: d} ,loss:{: f}, vali_rmse:{: f}'.format(it, train_loss, vali_rmse))\n",
    "\n",
    "            if last_vali_rmse and (last_vali_rmse - vali_rmse) <= 0:\n",
    "                print('convergence at iterations:{: d}'.format(it))\n",
    "                break\n",
    "            else:\n",
    "                last_vali_rmse = vali_rmse\n",
    "\n",
    "        return self.U, self.V, train_loss_list, vali_rmse_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "parameters are: ratio=0.6, reg_u=0.01, reg_v=0.01, latent_size=20, lr=3e-05, iters=1000\n",
      "traning iteration: 0 ,loss: 3813857.186933, vali_rmse: 6.583944\n",
      "traning iteration: 1 ,loss: 3804321.630254, vali_rmse: 6.575810\n",
      "traning iteration: 2 ,loss: 3790722.909650, vali_rmse: 6.564212\n",
      "traning iteration: 3 ,loss: 3773447.194736, vali_rmse: 6.549491\n",
      "traning iteration: 4 ,loss: 3752806.358725, vali_rmse: 6.531937\n",
      "traning iteration: 5 ,loss: 3729031.304247, vali_rmse: 6.511790\n",
      "traning iteration: 6 ,loss: 3702269.650224, vali_rmse: 6.489237\n",
      "traning iteration: 7 ,loss: 3672588.647574, vali_rmse: 6.464409\n",
      "traning iteration: 8 ,loss: 3639983.622504, vali_rmse: 6.437392\n",
      "traning iteration: 9 ,loss: 3604391.899214, vali_rmse: 6.408221\n",
      "traning iteration: 10 ,loss: 3565711.937038, vali_rmse: 6.376893\n",
      "traning iteration: 11 ,loss: 3523827.221521, vali_rmse: 6.343376\n",
      "traning iteration: 12 ,loss: 3478634.167392, vali_rmse: 6.307618\n",
      "traning iteration: 13 ,loss: 3430072.835287, vali_rmse: 6.269559\n",
      "traning iteration: 14 ,loss: 3378158.582057, vali_rmse: 6.229152\n",
      "traning iteration: 15 ,loss: 3323011.868821, vali_rmse: 6.186377\n",
      "traning iteration: 16 ,loss: 3264882.450766, vali_rmse: 6.141259\n",
      "traning iteration: 17 ,loss: 3204163.308428, vali_rmse: 6.093884\n",
      "traning iteration: 18 ,loss: 3141389.337370, vali_rmse: 6.044419\n",
      "traning iteration: 19 ,loss: 3077216.483169, vali_rmse: 5.993116\n",
      "traning iteration: 20 ,loss: 3012379.158350, vali_rmse: 5.940319\n",
      "traning iteration: 21 ,loss: 2947627.609519, vali_rmse: 5.886452\n",
      "traning iteration: 22 ,loss: 2883652.050024, vali_rmse: 5.832000\n",
      "traning iteration: 23 ,loss: 2821005.670782, vali_rmse: 5.777479\n",
      "traning iteration: 24 ,loss: 2760042.194086, vali_rmse: 5.723388\n",
      "traning iteration: 25 ,loss: 2700883.373878, vali_rmse: 5.670171\n",
      "traning iteration: 26 ,loss: 2643426.542160, vali_rmse: 5.618169\n",
      "traning iteration: 27 ,loss: 2587392.605873, vali_rmse: 5.567598\n",
      "traning iteration: 28 ,loss: 2532403.694796, vali_rmse: 5.518529\n",
      "traning iteration: 29 ,loss: 2478071.165919, vali_rmse: 5.470908\n",
      "traning iteration: 30 ,loss: 2424072.463639, vali_rmse: 5.424578\n",
      "traning iteration: 31 ,loss: 2370200.383118, vali_rmse: 5.379322\n",
      "traning iteration: 32 ,loss: 2316378.282800, vali_rmse: 5.334906\n",
      "traning iteration: 33 ,loss: 2262645.394597, vali_rmse: 5.291116\n",
      "traning iteration: 34 ,loss: 2209123.530425, vali_rmse: 5.247780\n",
      "traning iteration: 35 ,loss: 2155978.334644, vali_rmse: 5.204782\n",
      "traning iteration: 36 ,loss: 2103385.547944, vali_rmse: 5.162055\n",
      "traning iteration: 37 ,loss: 2051507.865702, vali_rmse: 5.119578\n",
      "traning iteration: 38 ,loss: 2000483.279107, vali_rmse: 5.077362\n",
      "traning iteration: 39 ,loss: 1950422.688697, vali_rmse: 5.035434\n",
      "traning iteration: 40 ,loss: 1901413.329319, vali_rmse: 4.993835\n",
      "traning iteration: 41 ,loss: 1853524.656310, vali_rmse: 4.952615\n",
      "traning iteration: 42 ,loss: 1806814.136375, vali_rmse: 4.911827\n",
      "traning iteration: 43 ,loss: 1761331.347917, vali_rmse: 4.871531\n",
      "traning iteration: 44 ,loss: 1717119.667996, vali_rmse: 4.831792\n",
      "traning iteration: 45 ,loss: 1674215.537328, vali_rmse: 4.792677\n",
      "traning iteration: 46 ,loss: 1632645.853525, vali_rmse: 4.754251\n",
      "traning iteration: 47 ,loss: 1592424.441356, vali_rmse: 4.716570\n",
      "traning iteration: 48 ,loss: 1553548.752308, vali_rmse: 4.679676\n",
      "traning iteration: 49 ,loss: 1515997.909646, vali_rmse: 4.643594\n",
      "traning iteration: 50 ,loss: 1479732.922647, vali_rmse: 4.608325\n",
      "traning iteration: 51 ,loss: 1444699.385919, vali_rmse: 4.573851\n",
      "traning iteration: 52 ,loss: 1410832.364757, vali_rmse: 4.540139\n",
      "traning iteration: 53 ,loss: 1378062.601663, vali_rmse: 4.507148\n",
      "traning iteration: 54 ,loss: 1346322.824312, vali_rmse: 4.474833\n",
      "traning iteration: 55 ,loss: 1315552.906170, vali_rmse: 4.443158\n",
      "traning iteration: 56 ,loss: 1285702.949452, vali_rmse: 4.412098\n",
      "traning iteration: 57 ,loss: 1256733.937611, vali_rmse: 4.381639\n",
      "traning iteration: 58 ,loss: 1228616.262853, vali_rmse: 4.351782\n",
      "traning iteration: 59 ,loss: 1201326.962171, vali_rmse: 4.322535\n",
      "traning iteration: 60 ,loss: 1174846.723695, vali_rmse: 4.293912\n",
      "traning iteration: 61 ,loss: 1149157.589308, vali_rmse: 4.265932\n",
      "traning iteration: 62 ,loss: 1124241.846338, vali_rmse: 4.238613\n",
      "traning iteration: 63 ,loss: 1100082.045073, vali_rmse: 4.211974\n",
      "traning iteration: 64 ,loss: 1076661.614838, vali_rmse: 4.186031\n",
      "traning iteration: 65 ,loss: 1053965.352209, vali_rmse: 4.160802\n",
      "traning iteration: 66 ,loss: 1031979.183802, vali_rmse: 4.136299\n",
      "traning iteration: 67 ,loss: 1010688.994399, vali_rmse: 4.112531\n",
      "traning iteration: 68 ,loss: 990078.789281, vali_rmse: 4.089498\n",
      "traning iteration: 69 ,loss: 970128.826704, vali_rmse: 4.067191\n",
      "traning iteration: 70 ,loss: 950814.462537, vali_rmse: 4.045589\n",
      "traning iteration: 71 ,loss: 932106.252328, vali_rmse: 4.024659\n",
      "traning iteration: 72 ,loss: 913971.436230, vali_rmse: 4.004365\n",
      "traning iteration: 73 ,loss: 896376.454309, vali_rmse: 3.984664\n",
      "traning iteration: 74 ,loss: 879289.786423, vali_rmse: 3.965517\n",
      "traning iteration: 75 ,loss: 862684.310306, vali_rmse: 3.946894\n",
      "traning iteration: 76 ,loss: 846538.551159, vali_rmse: 3.928769\n",
      "traning iteration: 77 ,loss: 830836.577214, vali_rmse: 3.911128\n",
      "traning iteration: 78 ,loss: 815566.730077, vali_rmse: 3.893961\n",
      "traning iteration: 79 ,loss: 800719.709251, vali_rmse: 3.877263\n",
      "traning iteration: 80 ,loss: 786286.651459, vali_rmse: 3.861028\n",
      "traning iteration: 81 ,loss: 772257.738436, vali_rmse: 3.845247\n",
      "traning iteration: 82 ,loss: 758621.598951, vali_rmse: 3.829909\n",
      "traning iteration: 83 ,loss: 745365.460592, vali_rmse: 3.815000\n",
      "traning iteration: 84 ,loss: 732475.773182, vali_rmse: 3.800502\n",
      "traning iteration: 85 ,loss: 719938.941641, vali_rmse: 3.786398\n",
      "traning iteration: 86 ,loss: 707741.875505, vali_rmse: 3.772670\n",
      "traning iteration: 87 ,loss: 695872.227451, vali_rmse: 3.759301\n",
      "traning iteration: 88 ,loss: 684318.367006, vali_rmse: 3.746277\n",
      "traning iteration: 89 ,loss: 673069.242251, vali_rmse: 3.733584\n",
      "traning iteration: 90 ,loss: 662114.288558, vali_rmse: 3.721208\n",
      "traning iteration: 91 ,loss: 651443.466950, vali_rmse: 3.709138\n",
      "traning iteration: 92 ,loss: 641047.408858, vali_rmse: 3.697363\n",
      "traning iteration: 93 ,loss: 630917.567827, vali_rmse: 3.685875\n",
      "traning iteration: 94 ,loss: 621046.267468, vali_rmse: 3.674664\n",
      "traning iteration: 95 ,loss: 611426.586792, vali_rmse: 3.663726\n",
      "traning iteration: 96 ,loss: 602052.105132, vali_rmse: 3.653053\n",
      "traning iteration: 97 ,loss: 592916.594595, vali_rmse: 3.642641\n",
      "traning iteration: 98 ,loss: 584013.766687, vali_rmse: 3.632484\n",
      "traning iteration: 99 ,loss: 575337.146989, vali_rmse: 3.622575\n",
      "traning iteration: 100 ,loss: 566880.089518, vali_rmse: 3.612910\n",
      "traning iteration: 101 ,loss: 558635.884611, vali_rmse: 3.603481\n",
      "traning iteration: 102 ,loss: 550597.888036, vali_rmse: 3.594281\n",
      "traning iteration: 103 ,loss: 542759.612198, vali_rmse: 3.585304\n",
      "traning iteration: 104 ,loss: 535114.759322, vali_rmse: 3.576542\n",
      "traning iteration: 105 ,loss: 527657.216584, vali_rmse: 3.567990\n",
      "traning iteration: 106 ,loss: 520381.052640, vali_rmse: 3.559639\n",
      "traning iteration: 107 ,loss: 513280.546179, vali_rmse: 3.551483\n",
      "traning iteration: 108 ,loss: 506350.248790, vali_rmse: 3.543516\n",
      "traning iteration: 109 ,loss: 499585.055214, vali_rmse: 3.535732\n",
      "traning iteration: 110 ,loss: 492980.241353, vali_rmse: 3.528124\n",
      "traning iteration: 111 ,loss: 486531.440971, vali_rmse: 3.520689\n",
      "traning iteration: 112 ,loss: 480234.559553, vali_rmse: 3.513423\n",
      "traning iteration: 113 ,loss: 474085.652924, vali_rmse: 3.506321\n",
      "traning iteration: 114 ,loss: 468080.813963, vali_rmse: 3.499380\n",
      "traning iteration: 115 ,loss: 462216.105743, vali_rmse: 3.492596\n",
      "traning iteration: 116 ,loss: 456487.557549, vali_rmse: 3.485966\n",
      "traning iteration: 117 ,loss: 450891.213191, vali_rmse: 3.479485\n",
      "traning iteration: 118 ,loss: 445423.201974, vali_rmse: 3.473151\n",
      "traning iteration: 119 ,loss: 440079.799271, vali_rmse: 3.466960\n",
      "traning iteration: 120 ,loss: 434857.455049, vali_rmse: 3.460909\n",
      "traning iteration: 121 ,loss: 429752.787199, vali_rmse: 3.454995\n",
      "traning iteration: 122 ,loss: 424762.552216, vali_rmse: 3.449215\n",
      "traning iteration: 123 ,loss: 419883.612014, vali_rmse: 3.443566\n",
      "traning iteration: 124 ,loss: 415112.911516, vali_rmse: 3.438047\n",
      "traning iteration: 125 ,loss: 410447.471697, vali_rmse: 3.432654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 126 ,loss: 405884.393284, vali_rmse: 3.427385\n",
      "traning iteration: 127 ,loss: 401420.862162, vali_rmse: 3.422237\n",
      "traning iteration: 128 ,loss: 397054.149508, vali_rmse: 3.417207\n",
      "traning iteration: 129 ,loss: 392781.605200, vali_rmse: 3.412293\n",
      "traning iteration: 130 ,loss: 388600.648094, vali_rmse: 3.407492\n",
      "traning iteration: 131 ,loss: 384508.758455, vali_rmse: 3.402800\n",
      "traning iteration: 132 ,loss: 380503.475892, vali_rmse: 3.398215\n",
      "traning iteration: 133 ,loss: 376582.402407, vali_rmse: 3.393733\n",
      "traning iteration: 134 ,loss: 372743.207258, vali_rmse: 3.389352\n",
      "traning iteration: 135 ,loss: 368983.629886, vali_rmse: 3.385068\n",
      "traning iteration: 136 ,loss: 365301.479061, vali_rmse: 3.380878\n",
      "traning iteration: 137 ,loss: 361694.629086, vali_rmse: 3.376779\n",
      "traning iteration: 138 ,loss: 358161.015587, vali_rmse: 3.372769\n",
      "traning iteration: 139 ,loss: 354698.633241, vali_rmse: 3.368844\n",
      "traning iteration: 140 ,loss: 351305.535987, vali_rmse: 3.365002\n",
      "traning iteration: 141 ,loss: 347979.838342, vali_rmse: 3.361242\n",
      "traning iteration: 142 ,loss: 344719.715489, vali_rmse: 3.357560\n",
      "traning iteration: 143 ,loss: 341523.400442, vali_rmse: 3.353956\n",
      "traning iteration: 144 ,loss: 338389.178303, vali_rmse: 3.350427\n",
      "traning iteration: 145 ,loss: 335315.379296, vali_rmse: 3.346971\n",
      "traning iteration: 146 ,loss: 332300.372933, vali_rmse: 3.343588\n",
      "traning iteration: 147 ,loss: 329342.565024, vali_rmse: 3.340275\n",
      "traning iteration: 148 ,loss: 326440.397796, vali_rmse: 3.337031\n",
      "traning iteration: 149 ,loss: 323592.351960, vali_rmse: 3.333855\n",
      "traning iteration: 150 ,loss: 320796.948929, vali_rmse: 3.330746\n",
      "traning iteration: 151 ,loss: 318052.751783, vali_rmse: 3.327702\n",
      "traning iteration: 152 ,loss: 315358.364543, vali_rmse: 3.324721\n",
      "traning iteration: 153 ,loss: 312712.430296, vali_rmse: 3.321803\n",
      "traning iteration: 154 ,loss: 310113.629177, vali_rmse: 3.318945\n",
      "traning iteration: 155 ,loss: 307560.676980, vali_rmse: 3.316148\n",
      "traning iteration: 156 ,loss: 305052.324575, vali_rmse: 3.313408\n",
      "traning iteration: 157 ,loss: 302587.357761, vali_rmse: 3.310726\n",
      "traning iteration: 158 ,loss: 300164.596940, vali_rmse: 3.308099\n",
      "traning iteration: 159 ,loss: 297782.896256, vali_rmse: 3.305526\n",
      "traning iteration: 160 ,loss: 295441.142202, vali_rmse: 3.303006\n",
      "traning iteration: 161 ,loss: 293138.252066, vali_rmse: 3.300538\n",
      "traning iteration: 162 ,loss: 290873.172652, vali_rmse: 3.298120\n",
      "traning iteration: 163 ,loss: 288644.879512, vali_rmse: 3.295752\n",
      "traning iteration: 164 ,loss: 286452.376665, vali_rmse: 3.293431\n",
      "traning iteration: 165 ,loss: 284294.696537, vali_rmse: 3.291157\n",
      "traning iteration: 166 ,loss: 282170.899851, vali_rmse: 3.288928\n",
      "traning iteration: 167 ,loss: 280080.075291, vali_rmse: 3.286744\n",
      "traning iteration: 168 ,loss: 278021.338976, vali_rmse: 3.284604\n",
      "traning iteration: 169 ,loss: 275993.833828, vali_rmse: 3.282506\n",
      "traning iteration: 170 ,loss: 273996.728973, vali_rmse: 3.280450\n",
      "traning iteration: 171 ,loss: 272029.219203, vali_rmse: 3.278434\n",
      "traning iteration: 172 ,loss: 270090.524464, vali_rmse: 3.276458\n",
      "traning iteration: 173 ,loss: 268179.889315, vali_rmse: 3.274521\n",
      "traning iteration: 174 ,loss: 266296.582337, vali_rmse: 3.272622\n",
      "traning iteration: 175 ,loss: 264439.895547, vali_rmse: 3.270760\n",
      "traning iteration: 176 ,loss: 262609.143863, vali_rmse: 3.268934\n",
      "traning iteration: 177 ,loss: 260803.664692, vali_rmse: 3.267144\n",
      "traning iteration: 178 ,loss: 259022.817620, vali_rmse: 3.265389\n",
      "traning iteration: 179 ,loss: 257265.984121, vali_rmse: 3.263668\n",
      "traning iteration: 180 ,loss: 255532.567221, vali_rmse: 3.261981\n",
      "traning iteration: 181 ,loss: 253821.991041, vali_rmse: 3.260326\n",
      "traning iteration: 182 ,loss: 252133.700224, vali_rmse: 3.258703\n",
      "traning iteration: 183 ,loss: 250467.159300, vali_rmse: 3.257112\n",
      "traning iteration: 184 ,loss: 248821.852038, vali_rmse: 3.255551\n",
      "traning iteration: 185 ,loss: 247197.280831, vali_rmse: 3.254021\n",
      "traning iteration: 186 ,loss: 245592.966129, vali_rmse: 3.252519\n",
      "traning iteration: 187 ,loss: 244008.445862, vali_rmse: 3.251047\n",
      "traning iteration: 188 ,loss: 242443.274847, vali_rmse: 3.249603\n",
      "traning iteration: 189 ,loss: 240897.024155, vali_rmse: 3.248186\n",
      "traning iteration: 190 ,loss: 239369.280434, vali_rmse: 3.246796\n",
      "traning iteration: 191 ,loss: 237859.645229, vali_rmse: 3.245433\n",
      "traning iteration: 192 ,loss: 236367.734314, vali_rmse: 3.244096\n",
      "traning iteration: 193 ,loss: 234893.177046, vali_rmse: 3.242784\n",
      "traning iteration: 194 ,loss: 233435.615720, vali_rmse: 3.241496\n",
      "traning iteration: 195 ,loss: 231994.704940, vali_rmse: 3.240233\n",
      "traning iteration: 196 ,loss: 230570.110958, vali_rmse: 3.238994\n",
      "traning iteration: 197 ,loss: 229161.511013, vali_rmse: 3.237779\n",
      "traning iteration: 198 ,loss: 227768.592655, vali_rmse: 3.236586\n",
      "traning iteration: 199 ,loss: 226391.053102, vali_rmse: 3.235416\n",
      "traning iteration: 200 ,loss: 225028.598613, vali_rmse: 3.234268\n",
      "traning iteration: 201 ,loss: 223680.943902, vali_rmse: 3.233142\n",
      "traning iteration: 202 ,loss: 222347.811599, vali_rmse: 3.232037\n",
      "traning iteration: 203 ,loss: 221028.931728, vali_rmse: 3.230952\n",
      "traning iteration: 204 ,loss: 219724.041240, vali_rmse: 3.229889\n",
      "traning iteration: 205 ,loss: 218432.883567, vali_rmse: 3.228845\n",
      "traning iteration: 206 ,loss: 217155.208232, vali_rmse: 3.227822\n",
      "traning iteration: 207 ,loss: 215890.770486, vali_rmse: 3.226817\n",
      "traning iteration: 208 ,loss: 214639.331004, vali_rmse: 3.225832\n",
      "traning iteration: 209 ,loss: 213400.655616, vali_rmse: 3.224866\n",
      "traning iteration: 210 ,loss: 212174.515075, vali_rmse: 3.223918\n",
      "traning iteration: 211 ,loss: 210960.684870, vali_rmse: 3.222988\n",
      "traning iteration: 212 ,loss: 209758.945081, vali_rmse: 3.222076\n",
      "traning iteration: 213 ,loss: 208569.080265, vali_rmse: 3.221182\n",
      "traning iteration: 214 ,loss: 207390.879398, vali_rmse: 3.220304\n",
      "traning iteration: 215 ,loss: 206224.135846, vali_rmse: 3.219444\n",
      "traning iteration: 216 ,loss: 205068.647371, vali_rmse: 3.218600\n",
      "traning iteration: 217 ,loss: 203924.216164, vali_rmse: 3.217772\n",
      "traning iteration: 218 ,loss: 202790.648897, vali_rmse: 3.216960\n",
      "traning iteration: 219 ,loss: 201667.756794, vali_rmse: 3.216164\n",
      "traning iteration: 220 ,loss: 200555.355709, vali_rmse: 3.215383\n",
      "traning iteration: 221 ,loss: 199453.266214, vali_rmse: 3.214618\n",
      "traning iteration: 222 ,loss: 198361.313689, vali_rmse: 3.213867\n",
      "traning iteration: 223 ,loss: 197279.328411, vali_rmse: 3.213131\n",
      "traning iteration: 224 ,loss: 196207.145638, vali_rmse: 3.212409\n",
      "traning iteration: 225 ,loss: 195144.605678, vali_rmse: 3.211702\n",
      "traning iteration: 226 ,loss: 194091.553950, vali_rmse: 3.211008\n",
      "traning iteration: 227 ,loss: 193047.841013, vali_rmse: 3.210328\n",
      "traning iteration: 228 ,loss: 192013.322588, vali_rmse: 3.209661\n",
      "traning iteration: 229 ,loss: 190987.859551, vali_rmse: 3.209008\n",
      "traning iteration: 230 ,loss: 189971.317902, vali_rmse: 3.208367\n",
      "traning iteration: 231 ,loss: 188963.568703, vali_rmse: 3.207739\n",
      "traning iteration: 232 ,loss: 187964.488001, vali_rmse: 3.207123\n",
      "traning iteration: 233 ,loss: 186973.956715, vali_rmse: 3.206520\n",
      "traning iteration: 234 ,loss: 185991.860497, vali_rmse: 3.205929\n",
      "traning iteration: 235 ,loss: 185018.089578, vali_rmse: 3.205349\n",
      "traning iteration: 236 ,loss: 184052.538581, vali_rmse: 3.204781\n",
      "traning iteration: 237 ,loss: 183095.106319, vali_rmse: 3.204225\n",
      "traning iteration: 238 ,loss: 182145.695579, vali_rmse: 3.203679\n",
      "traning iteration: 239 ,loss: 181204.212883, vali_rmse: 3.203145\n",
      "traning iteration: 240 ,loss: 180270.568245, vali_rmse: 3.202622\n",
      "traning iteration: 241 ,loss: 179344.674912, vali_rmse: 3.202109\n",
      "traning iteration: 242 ,loss: 178426.449108, vali_rmse: 3.201606\n",
      "traning iteration: 243 ,loss: 177515.809767, vali_rmse: 3.201114\n",
      "traning iteration: 244 ,loss: 176612.678274, vali_rmse: 3.200632\n",
      "traning iteration: 245 ,loss: 175716.978203, vali_rmse: 3.200160\n",
      "traning iteration: 246 ,loss: 174828.635074, vali_rmse: 3.199697\n",
      "traning iteration: 247 ,loss: 173947.576106, vali_rmse: 3.199244\n",
      "traning iteration: 248 ,loss: 173073.729990, vali_rmse: 3.198801\n",
      "traning iteration: 249 ,loss: 172207.026677, vali_rmse: 3.198367\n",
      "traning iteration: 250 ,loss: 171347.397179, vali_rmse: 3.197942\n",
      "traning iteration: 251 ,loss: 170494.773391, vali_rmse: 3.197526\n",
      "traning iteration: 252 ,loss: 169649.087930, vali_rmse: 3.197118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning iteration: 253 ,loss: 168810.273999, vali_rmse: 3.196720\n",
      "traning iteration: 254 ,loss: 167978.265262, vali_rmse: 3.196330\n",
      "traning iteration: 255 ,loss: 167152.995756, vali_rmse: 3.195948\n",
      "traning iteration: 256 ,loss: 166334.399805, vali_rmse: 3.195575\n",
      "traning iteration: 257 ,loss: 165522.411972, vali_rmse: 3.195210\n",
      "traning iteration: 258 ,loss: 164716.967019, vali_rmse: 3.194853\n",
      "traning iteration: 259 ,loss: 163917.999891, vali_rmse: 3.194504\n",
      "traning iteration: 260 ,loss: 163125.445718, vali_rmse: 3.194162\n",
      "traning iteration: 261 ,loss: 162339.239833, vali_rmse: 3.193829\n",
      "traning iteration: 262 ,loss: 161559.317802, vali_rmse: 3.193503\n",
      "traning iteration: 263 ,loss: 160785.615471, vali_rmse: 3.193184\n",
      "traning iteration: 264 ,loss: 160018.069025, vali_rmse: 3.192873\n",
      "traning iteration: 265 ,loss: 159256.615050, vali_rmse: 3.192569\n",
      "traning iteration: 266 ,loss: 158501.190606, vali_rmse: 3.192273\n",
      "traning iteration: 267 ,loss: 157751.733312, vali_rmse: 3.191983\n",
      "traning iteration: 268 ,loss: 157008.181421, vali_rmse: 3.191700\n",
      "traning iteration: 269 ,loss: 156270.473909, vali_rmse: 3.191425\n",
      "traning iteration: 270 ,loss: 155538.550555, vali_rmse: 3.191156\n",
      "traning iteration: 271 ,loss: 154812.352026, vali_rmse: 3.190894\n",
      "traning iteration: 272 ,loss: 154091.819951, vali_rmse: 3.190638\n",
      "traning iteration: 273 ,loss: 153376.897001, vali_rmse: 3.190389\n",
      "traning iteration: 274 ,loss: 152667.526947, vali_rmse: 3.190146\n",
      "traning iteration: 275 ,loss: 151963.654727, vali_rmse: 3.189910\n",
      "traning iteration: 276 ,loss: 151265.226493, vali_rmse: 3.189680\n",
      "traning iteration: 277 ,loss: 150572.189658, vali_rmse: 3.189456\n",
      "traning iteration: 278 ,loss: 149884.492921, vali_rmse: 3.189238\n",
      "traning iteration: 279 ,loss: 149202.086296, vali_rmse: 3.189027\n",
      "traning iteration: 280 ,loss: 148524.921124, vali_rmse: 3.188821\n",
      "traning iteration: 281 ,loss: 147852.950070, vali_rmse: 3.188621\n",
      "traning iteration: 282 ,loss: 147186.127123, vali_rmse: 3.188427\n",
      "traning iteration: 283 ,loss: 146524.407571, vali_rmse: 3.188239\n",
      "traning iteration: 284 ,loss: 145867.747977, vali_rmse: 3.188056\n",
      "traning iteration: 285 ,loss: 145216.106143, vali_rmse: 3.187879\n",
      "traning iteration: 286 ,loss: 144569.441065, vali_rmse: 3.187707\n",
      "traning iteration: 287 ,loss: 143927.712881, vali_rmse: 3.187541\n",
      "traning iteration: 288 ,loss: 143290.882810, vali_rmse: 3.187380\n",
      "traning iteration: 289 ,loss: 142658.913091, vali_rmse: 3.187225\n",
      "traning iteration: 290 ,loss: 142031.766908, vali_rmse: 3.187074\n",
      "traning iteration: 291 ,loss: 141409.408317, vali_rmse: 3.186929\n",
      "traning iteration: 292 ,loss: 140791.802169, vali_rmse: 3.186789\n",
      "traning iteration: 293 ,loss: 140178.914030, vali_rmse: 3.186653\n",
      "traning iteration: 294 ,loss: 139570.710093, vali_rmse: 3.186523\n",
      "traning iteration: 295 ,loss: 138967.157101, vali_rmse: 3.186398\n",
      "traning iteration: 296 ,loss: 138368.222260, vali_rmse: 3.186277\n",
      "traning iteration: 297 ,loss: 137773.873155, vali_rmse: 3.186161\n",
      "traning iteration: 298 ,loss: 137184.077670, vali_rmse: 3.186050\n",
      "traning iteration: 299 ,loss: 136598.803910, vali_rmse: 3.185943\n",
      "traning iteration: 300 ,loss: 136018.020120, vali_rmse: 3.185840\n",
      "traning iteration: 301 ,loss: 135441.694611, vali_rmse: 3.185743\n",
      "traning iteration: 302 ,loss: 134869.795693, vali_rmse: 3.185649\n",
      "traning iteration: 303 ,loss: 134302.291605, vali_rmse: 3.185560\n",
      "traning iteration: 304 ,loss: 133739.150453, vali_rmse: 3.185475\n",
      "traning iteration: 305 ,loss: 133180.340152, vali_rmse: 3.185395\n",
      "traning iteration: 306 ,loss: 132625.828374, vali_rmse: 3.185318\n",
      "traning iteration: 307 ,loss: 132075.582498, vali_rmse: 3.185246\n",
      "traning iteration: 308 ,loss: 131529.569570, vali_rmse: 3.185177\n",
      "traning iteration: 309 ,loss: 130987.756261, vali_rmse: 3.185113\n",
      "traning iteration: 310 ,loss: 130450.108837, vali_rmse: 3.185052\n",
      "traning iteration: 311 ,loss: 129916.593130, vali_rmse: 3.184995\n",
      "traning iteration: 312 ,loss: 129387.174517, vali_rmse: 3.184942\n",
      "traning iteration: 313 ,loss: 128861.817902, vali_rmse: 3.184893\n",
      "traning iteration: 314 ,loss: 128340.487704, vali_rmse: 3.184847\n",
      "traning iteration: 315 ,loss: 127823.147848, vali_rmse: 3.184805\n",
      "traning iteration: 316 ,loss: 127309.761765, vali_rmse: 3.184766\n",
      "traning iteration: 317 ,loss: 126800.292391, vali_rmse: 3.184731\n",
      "traning iteration: 318 ,loss: 126294.702173, vali_rmse: 3.184699\n",
      "traning iteration: 319 ,loss: 125792.953080, vali_rmse: 3.184670\n",
      "traning iteration: 320 ,loss: 125295.006618, vali_rmse: 3.184645\n",
      "traning iteration: 321 ,loss: 124800.823842, vali_rmse: 3.184623\n",
      "traning iteration: 322 ,loss: 124310.365382, vali_rmse: 3.184605\n",
      "traning iteration: 323 ,loss: 123823.591464, vali_rmse: 3.184589\n",
      "traning iteration: 324 ,loss: 123340.461935, vali_rmse: 3.184576\n",
      "traning iteration: 325 ,loss: 122860.936294, vali_rmse: 3.184567\n",
      "traning iteration: 326 ,loss: 122384.973722, vali_rmse: 3.184560\n",
      "traning iteration: 327 ,loss: 121912.533113, vali_rmse: 3.184556\n",
      "traning iteration: 328 ,loss: 121443.573111, vali_rmse: 3.184556\n",
      "traning iteration: 329 ,loss: 120978.052143, vali_rmse: 3.184558\n",
      "convergence at iterations: 329\n",
      "Testing model.\n",
      "Test RMSE: 3.1722388585512133\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import RandomState\n",
    "import copy\n",
    "\n",
    "lambda_alpha = 0.01\n",
    "lambda_beta = 0.01\n",
    "latent_size = 20\n",
    "lr = 3e-5\n",
    "iters = 1000\n",
    "model = PMF(R=R, lambda_alpha=lambda_alpha, lambda_beta=lambda_beta, latent_size=latent_size, momuntum=0.9, lr=lr, iters=iters, seed=1)\n",
    "print('parameters are: ratio={}, reg_u={}, reg_v={}, latent_size={}, lr={}, iters={}'.format(ratio, lambda_alpha, lambda_beta, latent_size,lr, iters))\n",
    "U, V, train_loss_list, vali_rmse_list = model.train(train_data=train_data, vali_data=vali_data)\n",
    "\n",
    "print('Testing model.')\n",
    "preds = model.predict(data=test_data)\n",
    "test_rmse = RMSE(preds, test_data[:, 2])\n",
    "\n",
    "print('Test RMSE: {}'.format(test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE for validation dataset is approximately similar to test RMSE, which is good!\n",
    "Unfortunatly, we have low density of dataset (0.31%) and we used only part of our dataset (for 1000 ids), so results could be more promising."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
